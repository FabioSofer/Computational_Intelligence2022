{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Lab 3: Policy Search\n",
    "\n",
    "## Task\n",
    "\n",
    "Write agents able to play [*Nim*](https://en.wikipedia.org/wiki/Nim), with an arbitrary number of rows and an upper bound $k$ on the number of objects that can be removed in a turn (a.k.a., *subtraction game*).\n",
    "\n",
    "The player **taking the last object wins**.\n",
    "\n",
    "* Task3.1: An agent using fixed rules based on *nim-sum* (i.e., an *expert system*)\n",
    "* Task3.2: An agent using evolved rules\n",
    "* Task3.3: An agent using minmax\n",
    "* Task3.4: An agent using reinforcement learning\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Create the directory `lab3` inside the course repo \n",
    "* Put a `README.md` and your solution (all the files, code and auxiliary data if needed)\n",
    "\n",
    "## Notes\n",
    "\n",
    "* Working in group is not only allowed, but recommended (see: [Ubuntu](https://en.wikipedia.org/wiki/Ubuntu_philosophy) and [Cooperative Learning](https://files.eric.ed.gov/fulltext/EJ1096789.pdf)). Collaborations must be explicitly declared in the `README.md`.\n",
    "* [Yanking](https://www.emacswiki.org/emacs/KillingAndYanking) from the internet is allowed, but sources must be explicitly declared in the `README.md`.\n",
    "\n",
    "## Deadlines ([AoE](https://en.wikipedia.org/wiki/Anywhere_on_Earth))\n",
    "\n",
    "* Sunday, December 4th for Task3.1 and Task3.2\n",
    "* Sunday, December 11th for Task3.3 and Task3.4\n",
    "* Sunday, December 18th for all reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import namedtuple\n",
    "import random\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "from itertools import accumulate, product\n",
    "from operator import xor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The *Nim* and *Agent* classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<\" + \" \".join(str(_) for _ in self._rows) + \">\"\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        rowList=list(self._rows)\n",
    "        rowList.sort()\n",
    "        return hash(\" \".join(str(_) for _ in self._rows))\n",
    "\n",
    "    def __eq__(self, __o: object) -> bool:\n",
    "        return (self.__hash__()==__o.__hash__())\n",
    "\n",
    "    def assign_rows(self, rows):\n",
    "        self._rows = list(rows)\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        self._rows[row] -= num_objects\n",
    "\n",
    "    def is_game_over(self):\n",
    "        if sum(o > 0 for o in self._rows)==1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_state_and_reward(self):\n",
    "        return self, self.get_reward()\n",
    "\n",
    "    def get_reward(self):\n",
    "        # if at end give 0 reward\n",
    "        # if not at end give -1 reward\n",
    "        return -1 * int(not self.is_game_over())\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, num_rows: int,  alpha=0.15, random_factor=0.2) -> None:\n",
    "        self.G = {}\n",
    "        self.alpha = alpha\n",
    "        self.random_factor = random_factor\n",
    "        self.state_history = []\n",
    "\n",
    "        tmp = []\n",
    "        for i in range(num_rows):\n",
    "            tmp.append(range(0, i * 2 + 2))\n",
    "        for i in list(product(*tmp)):\n",
    "            n = Nim(num_rows)\n",
    "            self.G[n.assign_rows(i)] = random.random()\n",
    "\n",
    "    def choose_action(self, board: Nim):\n",
    "        \"\"\" chooses action according to action policy \"\"\"\n",
    "        r = random.random()\n",
    "        possible = [(r, o) for r, c in enumerate(board.rows) for o in range(1,c+1)] \n",
    "\n",
    "        if r < self.random_factor: # for epsilon-greedy policy\n",
    "            next_move = random.choice(possible)\n",
    "        else:       # choose best possible action in this state\n",
    "            evaluations = list()\n",
    "            for ply in possible:\n",
    "                newBoard=deepcopy(board) #return new board\n",
    "                newBoard.nimming(ply)\n",
    "                evaluations.append((ply, self.G[newBoard]))\n",
    "\n",
    "            # we choose the action with the higher G\n",
    "            next_move = max(evaluations, key=lambda k: k[1])[0]\n",
    "\n",
    "        return next_move\n",
    "\n",
    "    def update_state_history(self, state, reward):\n",
    "        self.state_history.append((state, reward))\n",
    "\n",
    "    def learn(self):\n",
    "        target = 0\n",
    "\n",
    "        for prev, reward in reversed(self.state_history):\n",
    "            self.G[prev] = self.G[prev] + self.alpha * (target - self.G[prev])\n",
    "            target += reward\n",
    "\n",
    "        self.state_history = []\n",
    "\n",
    "        self.random_factor -= 10e-5  # decrease random factor each episode of play\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Strategies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_random(state: Nim) -> Nimply:\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    num_objects = random.randint(1, state.rows[row])\n",
    "    return Nimply(row, num_objects),0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabriele(state: Nim) -> Nimply:\n",
    "    \"\"\"Pick always the maximum possible number of the lowest row\"\"\"\n",
    "    possible_moves = [(r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1)]\n",
    "    return Nimply(*max(possible_moves, key=lambda m: (-m[0], m[1]))),0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def franchino(state: Nim) -> Nimply:\n",
    "    \"\"\"Pick always the one from the longest row\"\"\"\n",
    "    return Nimply(max(enumerate(state.rows), key=lambda a: a[1])[0], 1),0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P1: Expert Player (same as Professor's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_sum(state: Nim) -> int:\n",
    "    *_, result = accumulate(state.rows, xor)\n",
    "    return result\n",
    "\n",
    "\n",
    "def cook_status(state: Nim,nimSum=False) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = [\n",
    "        (r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k\n",
    "    ]\n",
    "    cooked[\"active_rows_number\"] = sum(o > 0 for o in state.rows)\n",
    "    cooked[\"shortest_row\"] = min((x for x in enumerate(state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "    cooked[\"longest_row\"] = max((x for x in enumerate(state.rows)), key=lambda y: y[1])[0]\n",
    "    if nimSum:\n",
    "        cooked[\"nim_sum\"] = nim_sum(state)\n",
    "\n",
    "        brute_force = list()\n",
    "        for m in cooked[\"possible_moves\"]:\n",
    "            tmp = deepcopy(state)\n",
    "            tmp.nimming(m)\n",
    "            brute_force.append((m, nim_sum(tmp)))\n",
    "        cooked[\"brute_force\"] = brute_force\n",
    "    return cooked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_startegy(state: Nim) -> Nimply:\n",
    "    data = cook_status(state,True)\n",
    "    return next((bf for bf in data[\"brute_force\"] if bf[1] == 0), random.choice(data[\"brute_force\"]))[0],0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:status: Initial board  -> <1 3 5 7 9 11 13>\n",
      "INFO:root:Win ration 0.12 won!\n",
      "INFO:root:Win ration 0.02 won!\n",
      "INFO:root:Win ration 0.06 won!\n",
      "INFO:root:Win ration 0.1 won!\n",
      "INFO:root:Win ration 0.02 won!\n",
      "INFO:root:Win ration 0.08 won!\n",
      "INFO:root:Win ration 0.02 won!\n",
      "INFO:root:Win ration 0.02 won!\n",
      "INFO:root:Win ration 0.04 won!\n",
      "INFO:root:Win ration 0.04 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.06 won!\n",
      "INFO:root:Win ration 0.02 won!\n",
      "INFO:root:Win ration 0.04 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.04 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.04 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.02 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.02 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.02 won!\n",
      "INFO:root:Win ration 0.08 won!\n",
      "INFO:root:Win ration 0.02 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.02 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.02 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.02 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n",
      "INFO:root:Win ration 0.0 won!\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "NUM_ROWS = 7\n",
    "opponent = gabriele\n",
    "\n",
    "board = Nim(NUM_ROWS)\n",
    "logging.debug(f\"status: Initial board  -> {board}\")\n",
    "\n",
    "jack = Agent(NUM_ROWS, alpha=0.3, random_factor=0.2)\n",
    "moveHistory = []\n",
    "indices = []\n",
    "win = 0\n",
    "\n",
    "for i in range(10000):\n",
    "    steps = 0\n",
    "    player = 0\n",
    "    while not board.is_game_over():\n",
    "        steps += 1\n",
    "        if player == 0:\n",
    "            # choose an action (explore or exploit)\n",
    "            action = jack.choose_action(board)\n",
    "            board.nimming(action)\n",
    "            reward = board.get_reward()  # get the new reward\n",
    "            # update the robot memory with state and reward\n",
    "            jack.update_state_history(board, reward)\n",
    "        else:\n",
    "            ply = opponent(board)[0]\n",
    "            board.nimming(ply)\n",
    "\n",
    "        if steps > 100:\n",
    "            # end the robot if it takes too long to find the goal\n",
    "            break\n",
    "\n",
    "        player = 1 - player\n",
    "\n",
    "        #if i % 100 == 0:\n",
    "            #logging.debug(f\"status: After player {player} -> {board}\")\n",
    "    \n",
    "    winner = 1 - player\n",
    "    if winner == 0:\n",
    "        win += 1\n",
    "\n",
    "    if i % 50 == 0 and i > 0:\n",
    "        win_ratio = win/50\n",
    "        win = 0\n",
    "        logging.info(f\"Win ration {win_ratio} won!\")\n",
    "\n",
    "    jack.learn()  # jack should learn after every episode\n",
    "    # get a history of number of steps taken to plot later\n",
    "    '''\n",
    "    if i % 50 == 0:\n",
    "        print(f\"{i}: {maze.steps}\")\n",
    "        moveHistory.append(maze.steps)\n",
    "        indices.append(i)\n",
    "    '''\n",
    "\n",
    "    board = Nim(NUM_ROWS)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "55ba9e9f377da4b122898c7da5dc65f5811df1165b58878575aa29f7eca42830"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
